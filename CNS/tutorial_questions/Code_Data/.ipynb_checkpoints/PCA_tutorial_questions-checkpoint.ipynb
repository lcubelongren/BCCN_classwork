{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dimensionality Reduction - PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be very challenging to generate hypotheses regarding either single neurons or the population when looking at high-dimensional population activity. Dimensionality reduction techniques can help by giving a low-dimensional summary of the high-dimensional population activity, and thus provide an efficient way to explore and visualise the data.\n",
    "\n",
    "The goal of this exercise is to learn how to apply PCA to neural data and how to interpret the results. \n",
    "We will start by analyzing a relatively simple dataset. \n",
    "\n",
    "The dataset was collected by [Graf *et al*, 2011](http://www.nature.com/neuro/journal/v14/n2/full/nn.2733.html).\n",
    "\n",
    "Details about the dataset:\n",
    "- Neural activity recorded from 65 V1 neurons using multi-electrode arrays\n",
    "- The subject was an anesthetized monkey. \n",
    "- Stimuli were drifing sinusoidal gratings of 0 and 90 degrees, randomly interleaved. \n",
    "- Each stimulus lasted 2560ms. The first 1280ms consisted of a grating, the second 1280 consisted of a blank screen.\n",
    "- The dataset contains 100 stimulus repetitions.\n",
    "- The neural activity is quantified by counting the number of spikes into 40 ms time bins. Each stimulus therefore has 64 time bins (2560/40).\n",
    "- The dataset you will work with is a small subset of the original dataset.\n",
    "\n",
    "\n",
    "If there is time left, we will try our hand at the  neuropixels dataset. This tutorial is inspired by exercises from Jonathan Pillow (see homework 1 of the course http://pillowlab.princeton.edu/teaching/statneuro2018/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.io import loadmat \n",
    "from sklearn.decomposition import PCA \n",
    "from numpy.linalg import eig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Visualize the data\n",
    "\n",
    "The data consist of a (6400, 65) matrix of binned spike counts. Each column constains the spike counts of one neuron, each row contains the spike counts in one time bin.\n",
    "\n",
    "**a.**\n",
    "Plot the population response during the first\n",
    "5 stimuli  (first 320 rows of X). Tip: see `plt.imshow()` to visualise the population response. The responses should show clear stimulus-locking. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of X: (6400, 65)\n"
     ]
    }
   ],
   "source": [
    "data = loadmat('v1data_Graf2011.mat')\n",
    "X = data['Msp']\n",
    "print('Dimensions of X:',X.shape)\n",
    "\n",
    "# Your code goes here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** Plot the responses of neurons 8 and 32 (columns 8 and 32) over the first 5 stimuli. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: What is the main difference in the response properties of neuron 8 and 32?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Their responses are anti-correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Investigate the dimensionality of the data using PCA\n",
    "\n",
    "Recall that PCA finds an ordered set of activity patterns (principal components) that explain most variance in the data. Mathematically, the principal components are the eigenvectors of the covariance matrix $X^T X/(n-1)$. The variance that they capture is measured by the corresponding eigenvalue. In practice, we don't have to work with eigenvectors but we can use the class `sklearn.decomposition.PCA`. Use the function `fit` and variable `pca.explained_variance_ratio_` to answer the following question. \n",
    "\n",
    "**a.**\n",
    "Fit PCA to the spike count data. Next, visualize the dimensionality of the data by making two figures.\n",
    "The first figure should show the fraction of variance explained. The second figure should show the cumulative sum of the fraction of variance explained. Note that both the x-axis should read 'PCs' for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# create an PCA object. \n",
    "# Giving it no input we won't reduce the data yet\n",
    "pca = PCA(n_components=None) \n",
    "\n",
    "# Your code goes here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: How many components are needed to account for 50% of the variance in the data? And for 90%?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.**\n",
    "Each principal component (PC) is a vector of length equal to the number of neurons. A PC can therefore be interpreted as an activity pattern, where the $i$th component of a PCs is the deviation of this neuron from its mean rate (PCA explains variance, so average deviation from the mean).\n",
    "\n",
    "Plot the first PC (The PCs are stored in the variable `pca.components_`). By definition, this is the single activity pattern that explains the most variance in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question:\n",
    " What do you notice about the sign of its elements? What does this tell you about the dominant activity pattern?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.** Plot the second PC. How do the values of neuron 8 and 32 (the neurons you previously looked at) compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.** Use the function `pca.transform` to transform the data. The result is again a (6400, 65) matrix. The first column contains the projection of the neural activity onto the first PC. This vector of length 6400 is the similarity of the population activity to the first PC, over time. Next, make a scatter plot of the first PC agains the second PC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question:\n",
    "     Can you speculate on what is going on here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.**\n",
    "Plot the first 320 time bins of PC 1 and PC 2 over time to get a final answer of what the first PCs could represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
