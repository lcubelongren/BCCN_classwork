{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Decoding\n",
    "\n",
    "Decoding has the objective to 'decode' the neural stimulus from the neural activity. This requires that the activity of neurons varies during stimulus presentation and therefore this approach is routinely used by neuroscientists to see if a neural population could be involved in processing a particular stimulus (or other behavioral variable). The classical approach is to train a model, i.e. classifier, on the neural activity such that you can use the model to decode the stimulus identity from 'unseen' activity patterns.\n",
    "\n",
    "The neural activity comes from an experiment where data was recorded from awake, head-fixed mouse using two neuropixel probes for 1.5 hours. See [link](http://data.cortexlab.net/dualPhase3/) for more details and the original dataset.\n",
    "\n",
    "Here are the relevant details of the data for the decoding task. \n",
    "\n",
    "* Neural data is the spike count binned in 100ms of 76 V1 neurons\n",
    "* During the recording, 17 different visual stimuli were shown to the mouse, each repeated 10 times in random order for 1.5 seconds (30 bins)\n",
    "* The stimuli are numbered with the first 16 numbers correspond to orientations 0, 22.5, ..., 337.5. Stimulus number 17 is darkness\n",
    "*  The data recording starting 0.5 seconds before stimulus onset until 0.5 seconds after stimulus offset.\n",
    "\n",
    "\n",
    "In a first step (2.1) we select the neural activity in the presence of the stimulus.\n",
    "Secondly (2.2) we provide example code to train a classifier.\n",
    "\n",
    "Your objective (2.3) is to decode the same neural data as in the example but by using Support Vector Machine (SVM) as a classifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# loading the data\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Loading the data\n",
    "\n",
    "The cell below loads an array X of size (trials, time points, neurons) containing the neural activity. The number of trials is 170 (10 repetitions per stimuli for 17 stimuli), the number of time points is 25 (0.5 seconds before,1.5 sec stimulus, 0.5 second after = 2.5 seconds with bins of 100 ms), and the number of neurons is 76. \n",
    "\n",
    "We further define an array Y of size (trials,) containing the stimulus identity of each trial. Since we have 170 trials we also have 170 stimulus identities. We will start by predicting the stimulus ($y$) from activity in the 1000-1100 ms bin after stimulus onset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('decoding_data.pickle', 'rb') as handle:\n",
    "    data_dict = pickle.load(handle)\n",
    "\n",
    "    \n",
    "X = data_dict['activity']\n",
    "y = data_dict['stimulus']\n",
    "trials, time_bins, neurons = X.shape\n",
    "n_stim = len(np.unique(y))\n",
    "\n",
    "print(\"Activity X: %u trials, %u bins, %u neurons\"%(trials, time_bins, neurons))\n",
    "\n",
    "print(\"Labels y: %u trials,  %u stimuli\"%(trials, n_stim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Classifiy the neural data according to stimulus identity\n",
    "### 2.2.1 Linear classifier as example\n",
    "Linear regression predicts a stimulus $y_i$ by a $\\hat{y}_i$, a weighted sum of neural activity:\n",
    "$$ \\hat{y_i} = \\sum_{j=1}^n \\beta_j x_{ij}.\n",
    "$$\n",
    "Here the sum is over the neurons $j=1,...,n$. The index $i$ denotes the sample. To fit a linear regression model means to find the $\\beta_i$'s that minimize the difference between the true stimulus and the estimated stimulus $\\hat{y}$:\n",
    "$$\n",
    "\\sum_i (y_i - \\hat{y}_i)^2  = \\sum_{i} (y_i - \\sum_j \\beta_j x_{ij})^2\n",
    "$$\n",
    "It turns out that simply minimizing the error is not what we want. Why? Because our linear regression will use any correlation between activities $x$ and stimuli $y$. Maybe there are some noisy neurons that just happen to be correlated with the stimulus by chance. To make sure our decoder only uses the neurons it needs, we penalize it for the weights $\\beta$ that are different from 0. Our objective now becomse\n",
    "$\\hat{y}$:\n",
    "$$\n",
    "\\sum_i (y_i - \\hat{y}_i)^2  + \\alpha \\sum_j \\beta_j^2.\n",
    "$$\n",
    "This version of linear regression is called ridge regression. \n",
    "By minimizing these two terms, we force the model to minize the error by assigning weights $\\beta$ to only those neurons it really needs. The 'tuning parameter' $\\alpha$ determines how important small weights are compared to the error minimization.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Applying the ridge regression classifier (naively)\n",
    "\n",
    "The following code fits ridge regression to the data using the the implementation from the scikit-learn library. \n",
    "See https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge for more information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "alpha = 10 # ad hoc choise of regularization strength\n",
    "t = 15 # activity in the 1000-1100 ms bin after stimulus onset.\n",
    "ridge = Ridge(alpha)\n",
    "ridge.fit(X[:,t], y)\n",
    "y_hat = ridge.predict(X[:,t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need a way of assessing how good our model does. We could use the squared error, but this isn't always\n",
    "easy to interpret or compare between datasets. We will therefore use the coefficient of determination $R^2.$ This measures the fraction of the variability in the stimulus that is explained by our prediction. If our classifier does perfect, the $R^2$ will be 1. If the classifier does only as good (or worse) compared to predicting the average stimulus, the $R^2$ will be 0 (or lower). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_R2(y, y_hat):\n",
    "    \"\"\" Fraction of variance explained\n",
    "        y, y_hat : arrays of size n_trials, )\n",
    "    \"\"\"\n",
    "    SS_tot = np.mean((y-y.mean())**2)\n",
    "    SS_reg = np.mean((y-y_hat)**2)\n",
    "    return 1-SS_reg/SS_tot\n",
    "print(\"R2: %0.2f\"%get_R2(y,y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We explained over half of the variability in the stimulus. Unfortunately there are two issues that need to be solved. The first issue is that we ad-hoc choose a value of or regularization strength  α\n",
    " . Maybe there are other values of  α\n",
    "\n",
    "  that allow us to do better! The second issue is that we trained and tested our model on the same dataset. This likely leads to overfitting, meaning a model that uses the noise particular to the activity pattterns in this dataset to predict the stimulus. It will therefore not generalize if the noise changes. We will now adress both issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Rigorous application of the ridge regression classifier by using training, validation and test sets\n",
    "\n",
    "**a. Split the data**\n",
    "\n",
    "First we will split the data into training, validation and test sets.The training subset is for training, while the validation subset is for assessing the performance of a model with a particular alpha value. Of course the best alpha value could be depending on the choise of the validation set. So to test the performance of the 'best performing' model we will need a third data set (test set).\n",
    "\n",
    "The following code splits the data into these three subsets. The first 102 trials (6 repeats of 17 stimulus presentations) comprise the training set, the next 34 trials (2 repeats of 17 stimulus presentations) comprise the validation set and the final 34 trials (2 repeats of 17 stimulus presentations) comprise the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 102\n",
    "n_valid = 34\n",
    "n_test = 34\n",
    "\n",
    "# choose the trials\n",
    "train_idx = np.arange(n_train)\n",
    "valid_idx = np.arange(n_train, n_train+n_valid)\n",
    "test_idx = np.arange(n_train+n_valid, trials)\n",
    "\n",
    "# Split the data\n",
    "X_train = X[train_idx]\n",
    "y_train = y[train_idx]\n",
    "X_valid = X[valid_idx]\n",
    "y_valid = y[valid_idx]\n",
    "X_test = X[test_idx]\n",
    "y_test = y[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b. Fitting a linear classifier** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 15 # activity in the 1000-1100 ms bin after stimulus onset.\n",
    "alphas = np.logspace(-1,3,100)\n",
    "R2s = np.zeros((len(alphas), ))\n",
    "for i, alpha in enumerate(alphas):\n",
    "    # Your code for fitting ridge regression goes here\n",
    "    # store validation shore in R2s[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c. Choose the optimal alpha value** \n",
    "\n",
    "Plot R2 as a function of alpha and choose the alpha that gives the highest R2 on the validation set. Use this to \n",
    "compute the performance on the train, validation and test set. Which is highest? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After you have fitted a ridge regression for each alpha,\n",
    "# this code will show their performance\n",
    "plt.semilogx(alphas, R2s)\n",
    "plt.xlabel('alphas')\n",
    "plt.ylabel('R2s')\n",
    "alpha_opt = alphas[np.argmax(R2s)]\n",
    "ridge = Ridge(alpha_opt)\n",
    "ridge.fit(X_train[:,t], y_train)\n",
    "y_train_hat = ridge.predict(X_train[:,t])\n",
    "y_valid_hat = ridge.predict(X_valid[:,t])\n",
    "y_test_hat = ridge.predict(X_test[:,t])\n",
    "print('R2 training set',get_R2(y_train, y_train_hat))\n",
    "print('R2 validation set',get_R2(y_valid, y_valid_hat))\n",
    "print('R2 test set',get_R2(y_test, y_test_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4 Rigorous application of the ridge regression classifier with k-fold cross-validation\n",
    "\n",
    "In the previous setup, we selected one subset of the data to train and another one to validate. This is a bit 'wasteful', since we are not using the validation data set for training our model. So we will change strategies and instead of splitting the data into training, validation and test sets, we only split the data into a training and test set. Just as before, we will use the test set to evaluate the performance of the model with the optimal alpha value.\n",
    "\n",
    "To make best use of the training data we will do k-fold cross-validation. If you are not familiar with cross-validation, then have a look at the [following](https://www.youtube.com/watch?v=TIgfjmp-4BA) video for a better intuition. Briefly we split the training set into $K$ different subsets or 'folds'. We cycle over the different folds, each time leaving one of them out as a validation set and training the model on the remaining folds. At the end, we have $K$ scores and take the average. We finally choose the $\\alpha$ that achieves the best average score. This may sound somewhat complicated at first. Luckily, `sklearn.linear_model` contains a class called `RidgeCV` that implements K-fold cross validation for us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have two split by merging the prevous train and validation set\n",
    "n_train = 102+34\n",
    "# choose the trials\n",
    "train_idx = np.arange(n_train)\n",
    "# Split the data\n",
    "X_train = X[train_idx]\n",
    "y_train = y[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let sklearn do the cross validation for us, for the same time point\n",
    "from sklearn.linear_model import RidgeCV\n",
    "t = 15\n",
    "K = 10 #The number of 'folds'\n",
    "ridge = RidgeCV(alphas, cv=K)\n",
    "ridge.fit(X_train[:,t], y_train)\n",
    "y_train_hat = ridge.predict(X_train[:,t])\n",
    "y_test_hat = ridge.predict(X_test[:,t])\n",
    "print('R2 training set',get_R2(y_train, y_train_hat))\n",
    "print('R2 test set',get_R2(y_test, y_test_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5 Decoding over time\n",
    "\n",
    "So far we only used the neural activity of one particular point in time (the 1000-1100 ms bin after stimulus onset) to decode the identity of the stimulus. However there could be other time points that encode more information about the stimulus. Remember, the stimulus is present from 0 to 1500 ms. In the next section we will evaluate which time point are most informative about the stimulus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "R2s = np.zeros((time_bins, ))\n",
    "for t in range(time_bins):\n",
    "    # Your code goes here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running this will show the performance over time\n",
    "time=np.linspace(0,2,time_bins)\n",
    "plt.plot(time, R2s)\n",
    "plt.hlines(0, time[0], time[-1],linestyles=\":\")\n",
    "plt.xlabel(\"Time (s)\", size=15)\n",
    "plt.ylabel(r\"$R^2$\", size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: use another decoder\n",
    "\n",
    "In the above example, we used a variant of linear regression, the simplest decoder possible. This was enough to decode the stimulus identity during stimulus presentation, but it is possible that another decoder can work better. For example, the set of stimuli is discrete so we could use a classifier such as an SVM. \n",
    "\n",
    "Tip: Most models don't have an Cross Validation (CV) extension (such as RidgeCV). However, you can use\n",
    "`from sklearn.model_selection.cross_val_score `. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "# there's no CV extension of SVC so u\n",
    "from sklearn.model_selection import cross_val_score \n",
    "c_vals = np.logspace(-1,2)\n",
    "c_opts = np.zeros((time_bins, ))\n",
    "accuracies = np.zeros((time_bins, ))\n",
    "\n",
    "for t in range(time_bins):\n",
    "    c_accuracies = np.zeros((len(c_vals),))\n",
    "    for i, c in enumerate(c_vals):\n",
    "        # Fit an SVM for C=c, store the performance in \n",
    "        # c_accuracies\n",
    "        \n",
    "    # choose best c and get test acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show classification accuracy over time\n",
    "\n",
    "plt.plot(time, accuracies, 'o-', label='accuracy')\n",
    "n_classes = len(np.unique(y_test))\n",
    "plt.hlines(1/n_classes, time[0], time[-1],linestyles=\":\", label='chance')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Time (m)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "plt.imshow(confusion_matrix(y_test, y_test_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Exercise: decode from single neurons\n",
    "\n",
    "Here we used the activity patterns of the whole population to decode the stimulus. You may be wondering how well a classifier would do when it only had access to a single neuron. Try this out by training a decoder to predict \n",
    "the stimulus `y` from the activity single neurons stored in the last dimension of `X`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
